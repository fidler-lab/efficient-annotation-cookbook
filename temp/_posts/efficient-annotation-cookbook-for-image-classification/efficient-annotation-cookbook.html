<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #20794d; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007ba5; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #007ba5; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #007ba5; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #20794d; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

  <!--radix_placeholder_meta_tags-->
  <title>Efficient Annotation Cookbook for Image Classification</title>

  <meta property="description" itemprop="description" content="Based on the paper &quot;Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets&quot;, accepted by CVPR21 as Oral presentation."/>


  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2021-04-06"/>
  <meta property="article:created" itemprop="dateCreated" content="2021-04-06"/>
  <meta name="article:author" content="Yuan-Hong Liao"/>
  <meta name="article:author" content="Amlan Kar"/>
  <meta name="article:author" content="Sanja Fidler"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Efficient Annotation Cookbook for Image Classification"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Based on the paper &quot;Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets&quot;, accepted by CVPR21 as Oral presentation."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Efficient Annotation Cookbook for Image Classification"/>
  <meta property="twitter:description" content="Based on the paper &quot;Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets&quot;, accepted by CVPR21 as Oral presentation."/>

  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Lean crowdsourcing: Combining humans and machines in an online system;citation_publication_date=2017;citation_author=Steve Branson;citation_author=Grant Van Horn;citation_author=Pietro Perona"/>
  <meta name="citation_reference" content="citation_title=Lean multiclass crowdsourcing;citation_publication_date=2018;citation_author=Grant Van Horn;citation_author=Steve Branson;citation_author=Scott Loarie;citation_author=Serge Belongie;citation_author=Pietro Perona"/>
  <meta name="citation_reference" content="citation_title=Transductive inference for text classification using support vector machines;citation_publication_date=1999;citation_author=Thorsten Joachims"/>
  <meta name="citation_reference" content="citation_title=Maximum likelihood estimation of observer error-rates using the EM algorithm;citation_publication_date=1979;citation_publisher=Wiley Online Library;citation_volume=28;citation_author=Alexander Philip Dawid;citation_author=Allan M Skene"/>
  <meta name="citation_reference" content="citation_title=Unsupervised representation learning by predicting image rotations;citation_publication_date=2018;citation_author=Spyros Gidaris;citation_author=Praveer Singh;citation_author=Nikos Komodakis"/>
  <meta name="citation_reference" content="citation_title=Bootstrap your own latent: A new approach to self-supervised learning;citation_publication_date=2020;citation_author=Jean-Bastien Grill;citation_author=Florian Strub;citation_author=Florent AltchÃ©;citation_author=Corentin Tallec;citation_author=Pierre H Richemond;citation_author=Elena Buchatskaya;citation_author=Carl Doersch;citation_author=Bernardo Avila Pires;citation_author=Zhaohan Daniel Guo;citation_author=Mohammad Gheshlaghi Azar;citation_author=Bootstrap your own latent: A new approach to self-supervised learning"/>
  <meta name="citation_reference" content="citation_title=Momentum contrast for unsupervised visual representation learning;citation_publication_date=2019;citation_author=Kaiming He;citation_author=Haoqi Fan;citation_author=Yuxin Wu;citation_author=Saining Xie;citation_author=Ross Girshick"/>
  <meta name="citation_reference" content="citation_title=A simple framework for contrastive learning of visual representations;citation_publication_date=2020;citation_author=Ting Chen;citation_author=Simon Kornblith;citation_author=Mohammad Norouzi;citation_author=Geoffrey Hinton"/>
  <meta name="citation_reference" content="citation_title=Unsupervised visual representation learning by context prediction;citation_publication_date=2015;citation_author=Carl Doersch;citation_author=Abhinav Gupta;citation_author=Alexei A Efros"/>
  <meta name="citation_reference" content="citation_title=Maximum likelihood estimation of observer error-rates using the EM algorithm;citation_publication_date=1979;citation_publisher=Wiley Online Library;citation_volume=28;citation_author=Alexander Philip Dawid;citation_author=Allan M Skene"/>
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","output","draft","bibliography","repository_url","preview"]}},"value":[{"type":"character","attributes":{},"value":["Efficient Annotation Cookbook for Image Classification"]},{"type":"character","attributes":{},"value":["Based on the paper \"Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets\", accepted by CVPR21 as Oral presentation.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation"]}},"value":[{"type":"character","attributes":{},"value":["Yuan-Hong Liao"]},{"type":"character","attributes":{},"value":["https://andrewliao11.github.io"]},{"type":"character","attributes":{},"value":["University of Toronto, Vector Institute"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation"]}},"value":[{"type":"character","attributes":{},"value":["Amlan Kar"]},{"type":"character","attributes":{},"value":["https://amlankar.github.io"]},{"type":"character","attributes":{},"value":["University of Toronto, Vector Institute, Nvidia"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation"]}},"value":[{"type":"character","attributes":{},"value":["Sanja Fidler"]},{"type":"character","attributes":{},"value":["http://www.cs.utoronto.ca/~fidler/"]},{"type":"character","attributes":{},"value":["University of Toronto, Vector Institute, Nvidia"]}]}]},{"type":"character","attributes":{},"value":["04-06-2021"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc","toc_depth","includes"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]},{"type":"integer","attributes":{},"value":[3]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["in_header"]}},"value":[{"type":"character","attributes":{},"value":["assets/teaser/teaser.html"]}]}]}]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["biblio.bib"]},{"type":"character","attributes":{},"value":["https://github.com/fidler-lab/efficient-annotation-cookbook"]},{"type":"character","attributes":{},"value":["assets/teaser/colored_grid.png"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["assets/paper_figures/plot_distraction_compare_cost_target_classes_precision.pdf","assets/paper_figures/plot_human_semi.pdf","assets/paper_figures/plot_imagenet100_top5.pdf","assets/paper_figures/plot_imagenet100.pdf","assets/paper_figures/plot_prior_change-10.pdf","assets/paper_figures/plot_prior_change-30.pdf","assets/paper_figures/plot_prototype_as_val.pdf","assets/paper_figures/plot_risk_change.pdf","assets/paper_figures/plot_self_none.pdf","assets/paper_figures/plot_semi_pitfall.pdf","assets/paper_figures/plot_semi_precision.pdf","assets/paper_figures/plot_semi.pdf","assets/paper_figures/plot_step_size_change.pdf","assets/paper_figures/plot_stopping.pdf","assets/paper_figures/plot_task_assignment_vis.pdf","assets/paper_figures/plot_task_assignment.pdf","assets/paper_figures/plot_unfinished.pdf","assets/paper_figures/plot_unfiorm_structured_noise_pseudolabel.pdf","assets/paper_figures/plot_worker_change.pdf","assets/teaser/colored_grid.png","assets/teaser/grid.png","assets/teaser/teaser.html","assets/thumbnail.png","biblio.bib","efficient-annotation-cookbook_files/anchor-4.2.2/anchor.min.js","efficient-annotation-cookbook_files/bowser-1.9.3/bowser.min.js","efficient-annotation-cookbook_files/distill-2.2.21/template.v2.js","efficient-annotation-cookbook_files/header-attrs-2.7/header-attrs.js","efficient-annotation-cookbook_files/jquery-1.11.3/jquery.min.js","efficient-annotation-cookbook_files/popper-2.6.0/popper.min.js","efficient-annotation-cookbook_files/tippy-6.2.7/tippy-bundle.umd.min.js","efficient-annotation-cookbook_files/tippy-6.2.7/tippy-light-border.css","efficient-annotation-cookbook_files/tippy-6.2.7/tippy.css","efficient-annotation-cookbook_files/tippy-6.2.7/tippy.umd.min.js","efficient-annotation-cookbook_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure img {
    width: 100%;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        var refHtml = $('#ref-' + $(this).attr('data-cites')).html();
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="efficient-annotation-cookbook_files/header-attrs-2.7/header-attrs.js"></script>
  <script src="efficient-annotation-cookbook_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="efficient-annotation-cookbook_files/popper-2.6.0/popper.min.js"></script>
  <link href="efficient-annotation-cookbook_files/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="efficient-annotation-cookbook_files/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="efficient-annotation-cookbook_files/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="efficient-annotation-cookbook_files/anchor-4.2.2/anchor.min.js"></script>
  <script src="efficient-annotation-cookbook_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="efficient-annotation-cookbook_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="efficient-annotation-cookbook_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->
  <!DOCTYPE html>
  <html lang="en" dir="ltr">
    <head>
      <meta charset="utf-8">
      <title></title>
      <script>
        /*mouseover & mouseout*/
        function setNewImage(){
          document.getElementById("main_image").src = "assets/teaser/colored_grid.png";
        }
        function setOldImage(){
          document.getElementById("main_image").src = "assets/teaser/grid.png";
        }
      </script>
      <style>
        .article-banner {
        width: auto;
        height: auto;
        border: 0;
        }
        img[src="assets/teaser/colored_grid.png"] {
          border: 0;
          padding: 0;
          width: 60%;
          display: block;
          margin-left: auto;
          margin-right: auto; 
          margin-top: 53px;
        }
        img[src="assets/teaser/grid.png"] {
          border: 0;
          padding: 0;
          width: 60%;
          display: block;
          margin-left: auto;
          margin-right: auto; 
          margin-top: 53px;
        }
        
        figcaption {
          color: grey;
          font-style: italic;
          padding: 2px;
          text-align: center;
        }
        .btn-container {
            width: 60%;
            display: flex;
            justify-content: center;
            margin-left: auto;
            margin-right: auto; 
            z-index: 1000;
            padding-top: 16px;
            position: relative;
        }
            
        .ourbutton {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 150px;
            height: 50px;
            border-radius: 10px;
            /*background-color: #ffffff1f;*/
            background-color:#236b90;
            border: solid 2px white;
            color: white;
            font-size: 18px;
            cursor: pointer;
            margin: 0 5px;
            user-select: none;
            text-align: center;
        }
        .ourbutton:hover {
            /*background-color: rgba(255, 255, 255, 0.17);*/
            background-color:#0F2E3D;
        }
        .ourbutton:after {
          background-color: darken(#0F2E3D, 20%);
        }
      </style>
    </head>
    <body>
      <div class="article-banner">
        <img id="main_image" onmouseover="setNewImage()" onmouseout="setOldImage()" src="assets/teaser/grid.png" class="article-banner";>
        <figcaption>Which images require more/fewer human annotation? Hover to see our model's opinion!</figcaption>
      </div>
      </a>
      <div class="btn-container">
        <div class="ourbutton" onclick="window.open('')">Paper (PDF)</div>
        <div class="ourbutton" onclick="window.open('')">Code</div>
        <div class="ourbutton" onclick="window.open('')">Video (TBA)</div>
        <!--div class="ourbutton" onclick="window.location="#dtbody"">Read â</div-->
      </div>
    </body>
  </html>


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Efficient Annotation Cookbook for Image Classification","description":"Based on the paper \"Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets\", accepted by CVPR21 as Oral presentation.","authors":[{"author":"Yuan-Hong Liao","authorURL":"https://andrewliao11.github.io","affiliation":"University of Toronto, Vector Institute","affiliationURL":"#","orcidID":""},{"author":"Amlan Kar","authorURL":"https://amlankar.github.io","affiliation":"University of Toronto, Vector Institute, Nvidia","affiliationURL":"#","orcidID":""},{"author":"Sanja Fidler","authorURL":"http://www.cs.utoronto.ca/~fidler/","affiliation":"University of Toronto, Vector Institute, Nvidia","affiliationURL":"#","orcidID":""}],"publishedDate":"2021-04-06T00:00:00.000+00:00","citationText":"Liao, et al., 2021"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Efficient Annotation Cookbook for Image Classification</h1>
<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>Based on the paper âTowards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets,â accepted by CVPR21 as Oral presentation.</p></p>
</div>

<div class="d-byline">
  Yuan-Hong Liao <a href="https://andrewliao11.github.io" class="uri">https://andrewliao11.github.io</a> (University of Toronto, Vector Institute)
  
,   Amlan Kar <a href="https://amlankar.github.io" class="uri">https://amlankar.github.io</a> (University of Toronto, Vector Institute, Nvidia)
  
,   Sanja Fidler <a href="http://www.cs.utoronto.ca/~fidler/" class="uri">http://www.cs.utoronto.ca/~fidler/</a> (University of Toronto, Vector Institute, Nvidia)
  
<br/>04-06-2021
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#laborious-annotation-process">Laborious Annotation Process</a></li>
<li><a href="#background-and-testbed">Background and Testbed</a>
<ul>
<li><a href="#how-to-aggregate-labels-ds-model">How to aggregate labels: DS model</a></li>
<li><a href="#testbed-imagenet100-sandbox">Testbed: ImageNet100 Sandbox</a></li>
</ul></li>
<li><a href="#matters-of-model-learning">Matters of Model Learning</a>
<ul>
<li><a href="#online-labeling-is-a-semi-supervised-problem">Online-Labeling is a Semi-Supervised Problem</a></li>
<li><a href="#self-supervised-learning-advances-online-labeling">Self-Supervised Learning Advances Online-Labeling</a></li>
<li><a href="#clean-validation-set-matters-in-accuracy-and-calibration-error">Clean Validation set Matters in Accuracy and Calibration Error</a></li>
</ul></li>
<li><a href="#matters-of-workers">Matters of Workers</a>
<ul>
<li><a href="#its-worth-using-gold-standard-question-sometimes">Itâs worth using Gold Standard Question sometimes</a></li>
<li><a href="#tradeoff-between-number-of-workers-and-time-cost">Tradeoff between number of workers and time cost</a></li>
</ul></li>
<li><a href="#matters-of-data">Matters of Data</a>
<ul>
<li><a href="#pre-filtering-dataset-to-some-extent">Pre-filtering Dataset to some extent</a></li>
<li><a href="#early-stopping-saves-you-some-money">Early Stopping Saves you some Money</a></li>
</ul></li>
<li><a href="#discussion">Discussion</a>
<ul>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#paper">Paper</a></li>
</ul></li>
</ul>
</nav>
</div>
<p>Data is the engine of modern computer vision, which necessitates collecting large-scale datasets. This is expensive, and guaranteeing the quality of the labels is a major challenge. We investigate <strong>efficient annotation strategies</strong> for collecting multi-class classification labels for a large collection of images.</p>
<h1 id="laborious-annotation-process">Laborious Annotation Process</h1>
<p>A common approach used in practice is to query humans to get a fixed number of labels per datum and aggregate them <span class="citation" data-cites="lin2014microsoft kaur2019foodx russakovsky2015imagenet">(<a href="#ref-lin2014microsoft" role="doc-biblioref">Lin et al. 2014</a>; <a href="#ref-kaur2019foodx" role="doc-biblioref">Kaur et al. 2019</a>; <a href="#ref-russakovsky2015imagenet" role="doc-biblioref">Russakovsky et al. 2015</a>)</span> presumably because of its simplicity and reliability. This can be prohibitively expensive and inefficient in human resource utilization for large datasets, as it assumes equal effort needed per datum.</p>
<h1 id="background-and-testbed">Background and Testbed</h1>
<h3 id="how-to-aggregate-labels-ds-model">How to aggregate labels: DS model</h3>
<p>The Dawid-Skene model views the annotation process as jointly inferring true labels and worker skills. The joint probability of true labels <span class="math inline">\(\mathcal{Y}\)</span>, annotations <span class="math inline">\(\mathcal{Z}\)</span>, and worker skills <span class="math inline">\(\mathcal{W}\)</span> is defined as the product of the prior of true labels and worker skills and the posterior of the annotations. We first define the notations, <span class="math inline">\(\mathcal{I_j}\)</span>: the images annotated by the <span class="math inline">\(j^{th}\)</span> worker, <span class="math inline">\(\mathcal{W_i}\)</span>: the workers that annotate the <span class="math inline">\(i^{th}\)</span> image, <span class="math inline">\(N\)</span>: the number of images, <span class="math inline">\(M\)</span>: the number of workers. Now, we can define the joint probability as <span class="math inline">\(P(\mathcal{Y}, \mathcal{Z}, \mathcal{W}) = \prod_{i \in [N]} p(y_i) \prod_{j \in [M]} p(w_j) \prod_{i, j \in \mathcal{W_i}} p(z_{ij} | y_i, w_j)\)</span>. In practice, inference is performed using expectation maximization, where parameters for images or workers are optimized at a time, <span class="math display">\[
\begin{align}
\bar{y_i} &amp;= \arg \max p(y_i) \prod_{j \in \mathcal{W_i}} p(z_{ij} | y_i, \bar{w_j}) \\
\bar{w_j} &amp;= \arg \max p(w_j) \prod_{i \in \mathcal{I_j}} p(z_{ij} | \bar{y_i}, w_j) \\
\end{align}
\]</span></p>
<p>Prior work<span class="citation" data-cites="Branson_2017_CVPR">(<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona 2017</a>)</span> moves to an online setting and improves DS model by using machine learning model predictions as image prior <span class="math inline">\(p(y_i)\)</span>, opening up the window of incorporating machine learner and DS model. However, they only perform experiments in a small-scale setting. We ask ourselves: <strong>How many annotations can we possibly reduce to annotate a large scale image classification dataset, such as ImageNet?</strong></p>
<h3 id="testbed-imagenet100-sandbox">Testbed: ImageNet100 Sandbox</h3>
<p>Evaluating and ablating multi-class label annotation efficiency at scale requires large datasets with diverse and relatively clean labels. We construct multiple subsets of the ImageNet dataset <span class="citation" data-cites="russakovsky2015imagenet">(<a href="#ref-russakovsky2015imagenet" role="doc-biblioref">Russakovsky et al. 2015</a>)</span> for our experiments. The following table shows the details of the different subsets in ImageNet100 Sandbox.</p>
<table>
<thead>
<tr class="header">
<th>Dataset</th>
<th style="text-align: center;">#Images</th>
<th style="text-align: center;">#Classes</th>
<th style="text-align: center;">Worker Acc.</th>
<th style="text-align: center;">Fine-Grained</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Commodity</td>
<td style="text-align: center;">20140</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td>Vertebrate</td>
<td style="text-align: center;">23220</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td>Insect + Fungus</td>
<td style="text-align: center;">16770</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">V</td>
</tr>
<tr class="even">
<td>Dog</td>
<td style="text-align: center;">22704</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">V</td>
</tr>
<tr class="odd">
<td>ImageNet100</td>
<td style="text-align: center;">125689</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">V</td>
</tr>
</tbody>
</table>
<p>Prior work <span class="citation" data-cites="hua2013collaborative long2015multi">(<a href="#ref-hua2013collaborative" role="doc-biblioref">Hua et al. 2013</a>; <a href="#ref-long2015multi" role="doc-biblioref">Long and Hua 2015</a>)</span> simulates workers as confusion matrices. Class confusion was modeled with symmetric uniform noise, which can result in <em>over-optimistic</em> performance estimates. Human annotators exhibit asymmetric and structured confusion <em>i.e.</em>, classes get confused with each other differently. In Fig.<a href="#fig:structured-workers">1</a>, we compare the number of annotations per image in simulation using uniform label noise vs.Â structured label noise that we crowdsource. We see significant gaps between the two. This arises particularly when using learnt models in the loop due to sensitivity to noisy labels coming from structured confusion in the workers. Therefore, we use simulated workers with structured noise in ImageNet100 Sandbox.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:structured-workers"></span>
<img src="assets/paper_figures/plot_unfiorm_structured_noise_pseudolabel.pdf" alt="Over-optimistic results from workers with uniform noise. Human workers tend to make *structured* mistakes. Simulated workers with uniform label noise (blue) can result in over-optimistic annotation performance. Experiments under workers with structured noise reflect real-life performance better."  />
<p class="caption">
Figure 1: Over-optimistic results from workers with uniform noise. Human workers tend to make <em>structured</em> mistakes. Simulated workers with uniform label noise (blue) can result in over-optimistic annotation performance. Experiments under workers with structured noise reflect real-life performance better.
</p>
</div>
</div>
<p>We simulate the process of annotating ImageNet100 and perform various ablation on the system, spanning from models, workers, and data itself. We end up being <strong>2.7x efficient (w/ 63% less annotations)</strong> w.r.t prior work <span class="citation" data-cites="Branson_2017_CVPR">(<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona 2017</a>)</span> and <strong>6.7x efficient (w/ 85% less annotations)</strong> w.r.t. manual annotations <span class="citation" data-cites="dawid1979maximum">(<a href="#ref-dawid1979maximum" role="doc-biblioref">Dawid and Skene 1979</a>)</span>.</p>
<p>In the following, we show how each component affects the final efficiency:</p>
<h1 id="matters-of-model-learning">Matters of Model Learning</h1>
<h3 id="online-labeling-is-a-semi-supervised-problem">Online-Labeling is a Semi-Supervised Problem</h3>
<p>During online-labeling, the goal is to infer true labels for all images in the dataset, making model learning akin to transductive learning <span class="citation" data-cites="joachims1999transductive">(<a href="#ref-joachims1999transductive" role="doc-biblioref">Joachims 1999</a>)</span>, where the test set is observed and can be used for learning. Thus, it is reasonable to expect efficiency gains if the datasetâs underlying structure is exploited by putting the unlabeled data to work, using semi-supervised learning. In Fig.<a href="#fig:semi-supervised">2</a> we perform <em>Peudolabel</em> and <em>Mixmatch</em> in online-labeling.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:semi-supervised"></span>
<img src="assets/paper_figures/plot_semi.pdf" alt="Incorporating semi-supervised approaches consistently increases efficiency. Note that semi-supervised learning does not have a significant boost in Dog subset due ot the poor worker quality (43%). When eyeballing the subset, we also find ineligible label errors in the Dog subset."  />
<p class="caption">
Figure 2: Incorporating semi-supervised approaches consistently increases efficiency. Note that semi-supervised learning does not have a significant boost in Dog subset due ot the poor worker quality (43%). When eyeballing the subset, we also find ineligible label errors in the Dog subset.
</p>
</div>
</div>
<h3 id="self-supervised-learning-advances-online-labeling">Self-Supervised Learning Advances Online-Labeling</h3>
<p>With recent advances in self-supervised learning, it is feasible to learn strong image feature extractors that rival supervised learning, using pretext tasks without any label. This allows learning in-domain feature extractors for annotation tasks, as opposed to using features pre-trained on ImageNet We compare the efficacy of using BYOL <span class="citation" data-cites="grill2020bootstrap">(<a href="#ref-grill2020bootstrap" role="doc-biblioref">Grill et al. 2020</a>)</span>, SimCLR <span class="citation" data-cites="chen2020simple">(<a href="#ref-chen2020simple" role="doc-biblioref">Chen et al. 2020</a>)</span>, MoCo <span class="citation" data-cites="he2019moco">(<a href="#ref-he2019moco" role="doc-biblioref">He et al. 2019</a>)</span>, relative location prediction <span class="citation" data-cites="doersch2015unsupervised">(<a href="#ref-doersch2015unsupervised" role="doc-biblioref">Doersch, Gupta, and Efros 2015</a>)</span> and rotation prediction <span class="citation" data-cites="gidaris2018unsupervised">(<a href="#ref-gidaris2018unsupervised" role="doc-biblioref">Gidaris, Singh, and Komodakis 2018</a>)</span> learnt on full ImageNet raw images as the feature extractor. In Fig.<a href="#fig:self-supervised">3</a>, we show that improvements in self-supervised learning consistently increase the efficiency for datasets with both fine and coarse-grained labels, with up to <strong>5x improvement</strong> at similar accuracy compared to not using a machine learning model in the loop (online DS).</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:self-supervised"></span>
<img src="assets/paper_figures/plot_self_none.pdf" alt="The improvements in self-supervised learning can be translated seemingly to online labeling. Note that no semi-supervised tricks are applied in this figure."  />
<p class="caption">
Figure 3: The improvements in self-supervised learning can be translated seemingly to online labeling. Note that no semi-supervised tricks are applied in this figure.
</p>
</div>
</div>
<h3 id="clean-validation-set-matters-in-accuracy-and-calibration-error">Clean Validation set Matters in Accuracy and Calibration Error</h3>
<p>The validation set plays an important role in online-labeling. It is used to perform model selection and model calibration. Prior work <span class="citation" data-cites="Branson_2017_CVPR">(<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona 2017</a>)</span> uses a modified cross-validation approach to generate model likelihoods. We find that this could underperform when the estimated labels are noisy, which pollutes the validation set and makes calibration challenging. Instead, we propose to use the clean prototype images as the validation set. In our paper, we use 10 prototype images per class. We perform 3-fold cross-validation in this experiment. When not using cross-validation, we either randomly select a subset as the validation set or use the (clean) prototype images as the validation set. In Fig.<a href="#fig:clean-validation">4</a>, we ablate the importance of having a clean validation set and performing cross-validation in terms of accuracy and expected calibration error on the most challenging subset, the Dog subset.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:clean-validation"></span>
<img src="assets/paper_figures/plot_prototype_as_val.pdf" alt="The validation set plays an important role of online-labeling. It is used to perform model selection and model calibration. We compare the importance of using clean examples as the validation set. When not using a clean validation set, the model tends to produce poorly calibrated probability (w/ calibration method applied [@guo2017calibration]), resulting in poor accuracy."  />
<p class="caption">
Figure 4: The validation set plays an important role of online-labeling. It is used to perform model selection and model calibration. We compare the importance of using clean examples as the validation set. When not using a clean validation set, the model tends to produce poorly calibrated probability (w/ calibration method applied <span class="citation" data-cites="guo2017calibration">(<a href="#ref-guo2017calibration" role="doc-biblioref">Guo et al. 2017</a>)</span>), resulting in poor accuracy.
</p>
</div>
</div>
<h1 id="matters-of-workers">Matters of Workers</h1>
<h3 id="its-worth-using-gold-standard-question-sometimes">Itâs worth using Gold Standard Question sometimes</h3>
<p>In reality, the requestor can ask gold standard questions or apply prior knowledge to design the prior <span class="math inline">\(p(w_j)\)</span>. We explore two possible prior <strong>A)</strong> Considering class identity and <strong>B)</strong> Considering worker identity. To consider the class identity, the task designer needs to have a clear thought of which classes are more difficult than others. To consider the workerâs identity, the task designer needs to query several gold standard questions from each worker. In Fig.<a href="#fig:prior-change">5</a>, we find that considering worker identity is especially useful for fine-grained datasets, such as Dog subset, improving 15 accuracy points in Dog, while in Commodity, the improvement is marginal.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:prior-change"></span>
<img src="assets/paper_figures/plot_prior_change-10.pdf" alt="For the fine-grained dataset, it is usually worth using gold standard questions to get a better prior over worker skills. The number appended in the legend denotes the prior strength."  />
<p class="caption">
Figure 5: For the fine-grained dataset, it is usually worth using gold standard questions to get a better prior over worker skills. The number appended in the legend denotes the prior strength.
</p>
</div>
</div>
<h3 id="tradeoff-between-number-of-workers-and-time-cost">Tradeoff between number of workers and time cost</h3>
<p>One way to speed up the dataset annotation process is to hire more workers at the same time. However, under a fixed number of total annotations, having more workers means having fewer observations for each worker, resulting in poor worker skill estimation. We explore this tradeoff by manipulating the number of workers involved in Fig.<a href="#fig:number-workers">6</a>. The gap is surprisingly high in the fine-grained dataset (14% accuracy points difference in Dog subset), while there is nearly no tradeoff in the coarse-grained dataset, such as Commodity subset.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:number-workers"></span>
<img src="assets/paper_figures/plot_worker_change.pdf" alt="Hiring more workers saves the total time to annotate a dataset, while it sacrifices the accuracy of the dataset sometimes. For the fine-grained dataset, the gap between using 10 workers and 1000 workers is around 14 accuracy points, while in the coarse-grained dataset, there is nearly no tradeoff."  />
<p class="caption">
Figure 6: Hiring more workers saves the total time to annotate a dataset, while it sacrifices the accuracy of the dataset sometimes. For the fine-grained dataset, the gap between using 10 workers and 1000 workers is around 14 accuracy points, while in the coarse-grained dataset, there is nearly no tradeoff.
</p>
</div>
</div>
<h1 id="matters-of-data">Matters of Data</h1>
<h3 id="pre-filtering-dataset-to-some-extent">Pre-filtering Dataset to some extent</h3>
<p>We have assumed that the requestor performs perfect filtering before annotation, <em>i.e.</em>, all the images to be annotated belong to the target classes, which does not always hold. We add an additional âNone of Theseâ class and ablate annotation efficiency in the presence of unfiltered images. We include different numbers of images from other classes and measure the mean precision with the number of annotations of the target classes. In Fig.<a href="#fig:distraction-image">7</a>, we see that even with 100% more images from irrelevant classes, we can retain comparable efficiency on a fine-grained dataset.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:distraction-image"></span>
<img src="assets/paper_figures/plot_distraction_compare_cost_target_classes_precision.pdf" alt="We intentionally add some irrelevance images from other classes to mimic the real-world cases. In our experiments, we find that even with 100% more images from irrelevant classes, we can still retain comparable efficiency." width="50%" />
<p class="caption">
Figure 7: We intentionally add some irrelevance images from other classes to mimic the real-world cases. In our experiments, we find that even with 100% more images from irrelevant classes, we can still retain comparable efficiency.
</p>
</div>
</div>
<h3 id="early-stopping-saves-you-some-money">Early Stopping Saves you some Money</h3>
<p>A clear criterion to stop annotation is when the unfinished set of images (images with estimated risk greater than a threshold) is empty. However, we observe that the annotation accuracy usually saturates and then grows slowly because of a small number of data points that are heavily confused by the pool of workers used. Therefore we suggest that the requestor 1) stop annotation at this time and separately annotate the small number of unfinished samples, possibly with expert annotators, and 2) set a maximum number of annotations per image. In Fig.<a href="#fig:early-stopping">8</a>, we show this is sufficient. We set the maximum annotations of each example to be 3 and early stop when the size of the finished set does not increase from its maximum value for 5 consecutive steps.</p>
<aside>
In online-labeling, we estimate the risk for each example at every step. If the exampleâs risk satisfies pre-defined threshold, we put them into finished set and will not query its annotation from workers at the next time step.
</aside>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:early-stopping"></span>
<img src="assets/paper_figures/plot_stopping.pdf" alt="We perform early stopping by monitoring the size of the finished set. This avoids over-sampling for confusing images and leaves the rest of them to expert workers if possible. Dashed lines represent the trajectories using stopping criterion from prior work [@Branson_2017_CVPR]" width="50%" />
<p class="caption">
Figure 8: We perform early stopping by monitoring the size of the finished set. This avoids over-sampling for confusing images and leaves the rest of them to expert workers if possible. Dashed lines represent the trajectories using stopping criterion from prior work <span class="citation" data-cites="Branson_2017_CVPR">(<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona 2017</a>)</span>
</p>
</div>
</div>
<h1 id="discussion">Discussion</h1>
<p>We presented improved online-labeling methods for large multi-class datasets. In a realistically simulated experiment with 125k images and 100 labels from ImageNet, we observe a 2.7x reduction in annotations required w.r.t. prior work to achieve 80% top-1 label accuracy. Our framework goes on to achieve 87.4% top-1 accuracy at 0.98 labels per image. Along with our improvements, we leave open questions for future research. 1) Our simulation is not perfect and does not consider individual image difficulty, instead only modeling class confusion. 2) How does one accelerate labeling beyond semantic classes, such as classifying the viewing angle of a car? 3) ImageNet has a clear label hierarchy, which can be utilized to achieve orthogonal gains <span class="citation" data-cites="van2018lean">(<a href="#ref-van2018lean" role="doc-biblioref">Van Horn et al. 2018</a>)</span> in the worker skill estimation 4) Going beyond classification is possible with the proposed model by appropriately modeling annotation likelihood as demonstrated in <span class="citation" data-cites="Branson_2017_CVPR">(<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona 2017</a>)</span>. However, accelerating these with learning in the loop requires specific attention to detail per task, which is an exciting avenue for future work. 5) Finally, we discussed annotation at scale, where improvements in learning help significantly. How can these be translated to small datasets?</p>
<h2 class="appendix" id="acknowledgments">Acknowledgments</h2>
<p>This work was supported by ERA, NSERC, and DARPA XAI. SF acknowledges the Canada CIFAR AI Chair award at the Vector Institute.</p>
<p>This webpage is based on Distill Template generated from <a href="https://rstudio.github.io/distill/">here</a></p>
<h2 class="appendix" id="paper">Paper</h2>
<div class="layout-chunk" data-layout="l-body-outset">
<p><img src="assets/thumbnail.png" width="885" /></p>
</div>
<p><strong>This paper is accepted to CVPR2021 as Oral presentations.</strong></p>
<p>If you find this article useful or you use our code, please consider cite:</p>
<pre><code>@inproceedings{good_practices,
    title={Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets},
    author={Yuan-Hong Liao and Amlan Kar and Sanja Fidler},
    booktitle={CVPR},
    year={2021}
    }</code></pre>
<div class="sourceCode" id="cb2"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Branson_2017_CVPR" class="csl-entry" role="doc-biblioentry">
Branson, Steve, Grant Van Horn, and Pietro Perona. 2017. <span>âLean Crowdsourcing: Combining Humans and Machines in an Online System.â</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
</div>
<div id="ref-chen2020simple" class="csl-entry" role="doc-biblioentry">
Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. <span>âA Simple Framework for Contrastive Learning of Visual Representations.â</span> <em>arXiv Preprint arXiv:2002.05709</em>.
</div>
<div id="ref-dawid1979maximum" class="csl-entry" role="doc-biblioentry">
Dawid, Alexander Philip, and Allan M Skene. 1979. <span>âMaximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm.â</span> <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em> 28 (1): 20â28.
</div>
<div id="ref-doersch2015unsupervised" class="csl-entry" role="doc-biblioentry">
Doersch, Carl, Abhinav Gupta, and Alexei A Efros. 2015. <span>âUnsupervised Visual Representation Learning by Context Prediction.â</span> In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 1422â30.
</div>
<div id="ref-gidaris2018unsupervised" class="csl-entry" role="doc-biblioentry">
Gidaris, Spyros, Praveer Singh, and Nikos Komodakis. 2018. <span>âUnsupervised Representation Learning by Predicting Image Rotations.â</span> <em>arXiv Preprint arXiv:1803.07728</em>.
</div>
<div id="ref-grill2020bootstrap" class="csl-entry" role="doc-biblioentry">
Grill, Jean-Bastien, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020. <span>âBootstrap Your Own Latent: A New Approach to Self-Supervised Learning.â</span> <em>arXiv Preprint arXiv:2006.07733</em>.
</div>
<div id="ref-guo2017calibration" class="csl-entry" role="doc-biblioentry">
Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. <span>âOn Calibration of Modern Neural Networks.â</span> <em>arXiv Preprint arXiv:1706.04599</em>.
</div>
<div id="ref-he2019moco" class="csl-entry" role="doc-biblioentry">
He, Kaiming, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2019. <span>âMomentum Contrast for Unsupervised Visual Representation Learning.â</span> <em>arXiv Preprint arXiv:1911.05722</em>.
</div>
<div id="ref-hua2013collaborative" class="csl-entry" role="doc-biblioentry">
Hua, Gang, Chengjiang Long, Ming Yang, and Yan Gao. 2013. <span>âCollaborative Active Learning of a Kernel Machine Ensemble for Recognition.â</span> In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 1209â16.
</div>
<div id="ref-joachims1999transductive" class="csl-entry" role="doc-biblioentry">
Joachims, Thorsten. 1999. <span>âTransductive Inference for Text Classification Using Support Vector Machines.â</span> In <em>ICML</em>, 200â209.
</div>
<div id="ref-kaur2019foodx" class="csl-entry" role="doc-biblioentry">
Kaur, Parneet, Karan Sikka, Weijun Wang, Serge Belongie, and Ajay Divakaran. 2019. <span>âFoodx-251: A Dataset for Fine-Grained Food Classification.â</span> <em>arXiv Preprint arXiv:1907.06167</em>.
</div>
<div id="ref-lin2014microsoft" class="csl-entry" role="doc-biblioentry">
Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C Lawrence Zitnick. 2014. <span>âMicrosoft Coco: Common Objects in Context.â</span> In <em>European Conference on Computer Vision</em>, 740â55. Springer.
</div>
<div id="ref-long2015multi" class="csl-entry" role="doc-biblioentry">
Long, Chengjiang, and Gang Hua. 2015. <span>âMulti-Class Multi-Annotator Active Learning with Robust Gaussian Process for Visual Recognition.â</span> In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 2839â47.
</div>
<div id="ref-russakovsky2015imagenet" class="csl-entry" role="doc-biblioentry">
Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. <span>âImageNet Large Scale Visual Recognition Challenge.â</span> <a href="http://arxiv.org/abs/1409.0575">http://arxiv.org/abs/1409.0575</a>.
</div>
<div id="ref-van2018lean" class="csl-entry" role="doc-biblioentry">
Van Horn, Grant, Steve Branson, Scott Loarie, Serge Belongie, and Pietro Perona. 2018. <span>âLean Multiclass Crowdsourcing.â</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2714â23.
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="references">References</h3>
<div id="references-listing"></div>
<h3 id="updates-and-corrections">Corrections</h3>
<p>If you see mistakes or want to suggest changes, please <a href="https://github.com/fidler-lab/efficient-annotation-cookbook/issues/new">create an issue</a> on the source repository.</p>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
