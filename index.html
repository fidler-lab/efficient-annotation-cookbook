<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q7EHZZ111E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-Q7EHZZ111E');
  </script>

  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #20794d; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007ba5; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #007ba5; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #007ba5; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #20794d; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

  <!--radix_placeholder_meta_tags-->
<title>Towards Efficient Annotation: Efficient Annotation Cookbook for Image Classification</title>

<meta property="description" itemprop="description" content="Based on the paper &quot;Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets&quot;, accepted by CVPR21 as Oral presentation."/>

<link rel="canonical" href="efficient-annotation.github.io/posts/efficient-annotation-cookbook-for-image-classification/"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2021-04-06"/>
<meta property="article:created" itemprop="dateCreated" content="2021-04-06"/>
<meta name="article:author" content="Yuan-Hong Liao"/>
<meta name="article:author" content="Amlan Kar"/>
<meta name="article:author" content="Sanja Fidler"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="Towards Efficient Annotation: Efficient Annotation Cookbook for Image Classification"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Based on the paper &quot;Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets&quot;, accepted by CVPR21 as Oral presentation."/>
<meta property="og:url" content="efficient-annotation.github.io/posts/efficient-annotation-cookbook-for-image-classification/"/>
<meta property="og:image" content="efficient-annotation.github.io/posts/efficient-annotation-cookbook-for-image-classification/assets/teaser/colored_grid.png"/>
<meta property="og:image:width" content="1050"/>
<meta property="og:image:height" content="760"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="Towards Efficient Annotation"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="Towards Efficient Annotation: Efficient Annotation Cookbook for Image Classification"/>
<meta property="twitter:description" content="Based on the paper &quot;Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets&quot;, accepted by CVPR21 as Oral presentation."/>
<meta property="twitter:url" content="efficient-annotation.github.io/posts/efficient-annotation-cookbook-for-image-classification/"/>
<meta property="twitter:image" content="efficient-annotation.github.io/posts/efficient-annotation-cookbook-for-image-classification/assets/teaser/colored_grid.png"/>
<meta property="twitter:image:width" content="1050"/>
<meta property="twitter:image:height" content="760"/>

<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Lean crowdsourcing: Combining humans and machines in an online system;citation_publication_date=2017;citation_author=Steve Branson;citation_author=Grant Van Horn;citation_author=Pietro Perona"/>
  <meta name="citation_reference" content="citation_title=Lean multiclass crowdsourcing;citation_publication_date=2018;citation_author=Grant Van Horn;citation_author=Steve Branson;citation_author=Scott Loarie;citation_author=Serge Belongie;citation_author=Pietro Perona"/>
  <meta name="citation_reference" content="citation_title=Transductive inference for text classification using support vector machines;citation_publication_date=1999;citation_author=Thorsten Joachims"/>
  <meta name="citation_reference" content="citation_title=Maximum likelihood estimation of observer error-rates using the EM algorithm;citation_publication_date=1979;citation_publisher=Wiley Online Library;citation_volume=28;citation_author=Alexander Philip Dawid;citation_author=Allan M Skene"/>
  <meta name="citation_reference" content="citation_title=Unsupervised representation learning by predicting image rotations;citation_publication_date=2018;citation_author=Spyros Gidaris;citation_author=Praveer Singh;citation_author=Nikos Komodakis"/>
  <meta name="citation_reference" content="citation_title=Bootstrap your own latent: A new approach to self-supervised learning;citation_publication_date=2020;citation_author=Jean-Bastien Grill;citation_author=Florian Strub;citation_author=Florent AltchÃ©;citation_author=Corentin Tallec;citation_author=Pierre H Richemond;citation_author=Elena Buchatskaya;citation_author=Carl Doersch;citation_author=Bernardo Avila Pires;citation_author=Zhaohan Daniel Guo;citation_author=Mohammad Gheshlaghi Azar;citation_author=Bootstrap your own latent: A new approach to self-supervised learning"/>
  <meta name="citation_reference" content="citation_title=Momentum contrast for unsupervised visual representation learning;citation_publication_date=2019;citation_author=Kaiming He;citation_author=Haoqi Fan;citation_author=Yuxin Wu;citation_author=Saining Xie;citation_author=Ross Girshick"/>
  <meta name="citation_reference" content="citation_title=A simple framework for contrastive learning of visual representations;citation_publication_date=2020;citation_author=Ting Chen;citation_author=Simon Kornblith;citation_author=Mohammad Norouzi;citation_author=Geoffrey Hinton"/>
  <meta name="citation_reference" content="citation_title=Unsupervised visual representation learning by context prediction;citation_publication_date=2015;citation_author=Carl Doersch;citation_author=Abhinav Gupta;citation_author=Alexei A Efros"/>
  <meta name="citation_reference" content="citation_title=Maximum likelihood estimation of observer error-rates using the EM algorithm;citation_publication_date=1979;citation_publisher=Wiley Online Library;citation_volume=28;citation_author=Alexander Philip Dawid;citation_author=Allan M Skene"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","output","draft","bibliography","repository_url","preview","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["Efficient Annotation Cookbook for Image Classification"]},{"type":"character","attributes":{},"value":["Based on the paper \"Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets\", accepted by CVPR21 as Oral presentation."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation"]}},"value":[{"type":"character","attributes":{},"value":["Yuan-Hong Liao"]},{"type":"character","attributes":{},"value":["https://andrewliao11.github.io"]},{"type":"character","attributes":{},"value":["University of Toronto, Vector Institute"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation"]}},"value":[{"type":"character","attributes":{},"value":["Amlan Kar"]},{"type":"character","attributes":{},"value":["https://amlankar.github.io"]},{"type":"character","attributes":{},"value":["University of Toronto, Vector Institute, Nvidia"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","affiliation"]}},"value":[{"type":"character","attributes":{},"value":["Sanja Fidler"]},{"type":"character","attributes":{},"value":["http://www.cs.utoronto.ca/~fidler/"]},{"type":"character","attributes":{},"value":["University of Toronto, Vector Institute, Nvidia"]}]}]},{"type":"character","attributes":{},"value":["04-06-2021"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc","toc_depth","includes"]}},"value":[{"type":"logical","attributes":{},"value":[false]},{"type":"logical","attributes":{},"value":[true]},{"type":"integer","attributes":{},"value":[3]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["in_header"]}},"value":[{"type":"character","attributes":{},"value":["assets/teaser/teaser.html"]}]}]}]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["biblio.bib"]},{"type":"character","attributes":{},"value":["https://github.com/fidler-lab/efficient-annotation-cookbook"]},{"type":"character","attributes":{},"value":["assets/teaser/colored_grid.png"]},{"type":"character","attributes":{},"value":["efficient-annotation.github.io/posts/efficient-annotation-cookbook-for-image-classification/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["assets/paper_figures/imagenet_split_4-proability_change.png","assets/paper_figures/pdf2png.py","assets/paper_figures/plot_distraction_compare_cost_target_classes_precision.png","assets/paper_figures/plot_human_semi.png","assets/paper_figures/plot_imagenet100_top5.png","assets/paper_figures/plot_imagenet100.png","assets/paper_figures/plot_prior_change-10.png","assets/paper_figures/plot_prior_change-30.png","assets/paper_figures/plot_prototype_as_val.png","assets/paper_figures/plot_risk_change.png","assets/paper_figures/plot_self_none.png","assets/paper_figures/plot_semi_pitfall.png","assets/paper_figures/plot_semi_precision.png","assets/paper_figures/plot_semi.png","assets/paper_figures/plot_step_size_change.png","assets/paper_figures/plot_stopping.png","assets/paper_figures/plot_task_assignment_vis.png","assets/paper_figures/plot_task_assignment.png","assets/paper_figures/plot_unfinished.png","assets/paper_figures/plot_unfiorm_structured_noise_pseudolabel.png","assets/paper_figures/plot_worker_change.png","assets/teaser/colored_grid.png","assets/teaser/grid.png","assets/teaser/teaser.html","assets/thumbnail.png","biblio.bib","efficient-annotation-cookbook_files/anchor-4.2.2/anchor.min.js","efficient-annotation-cookbook_files/bowser-1.9.3/bowser.min.js","efficient-annotation-cookbook_files/distill-2.2.21/template.v2.js","efficient-annotation-cookbook_files/header-attrs-2.7/header-attrs.js","efficient-annotation-cookbook_files/jquery-1.11.3/jquery.min.js","efficient-annotation-cookbook_files/popper-2.6.0/popper.min.js","efficient-annotation-cookbook_files/tippy-6.2.7/tippy-bundle.umd.min.js","efficient-annotation-cookbook_files/tippy-6.2.7/tippy-light-border.css","efficient-annotation-cookbook_files/tippy-6.2.7/tippy.css","efficient-annotation-cookbook_files/tippy-6.2.7/tippy.umd.min.js","efficient-annotation-cookbook_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="temp/site_libs/jquery-1.11.3/jquery.min.js"></script>
<link href="temp/site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="temp/site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="temp/site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="temp/site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="temp/site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createFuseIndex() {

  // create fuse index
  var options = {
    keys: [
      { name: 'title', weight: 20 },
      { name: 'categories', weight: 15 },
      { name: 'description', weight: 10 },
      { name: 'contents', weight: 5 },
    ],
    ignoreLocation: true,
    threshold: 0
  };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var img = suggestion.preview && Object.keys(suggestion.preview).length > 0
              ? `<img src="${offsetURL(suggestion.preview)}"</img>`
              : '';
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description || ''}
                </div>
                <div class="search-item-preview">
                  ${img}
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

.nav-search {
  font-size: x-small;
}

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  font-size: 1rem;
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      var refHtml = $('#ref-' + $(this).attr('data-cites')).html();
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<style type="text/css">
/* base variables */

/* Edit the CSS properties in this file to create a custom
   Distill theme. Only edit values in the right column
   for each row; values shown are the CSS defaults.
   To return any property to the default,
   you may set its value to: unset
   All rows must end with a semi-colon.                      */

/* Optional: embed custom fonts here with `@import`          */
/* This must remain at the top of this file.                 */



html {
  /*-- Main font sizes --*/
  --title-size:      50px;
  --body-size:       1.06rem;
  --code-size:       14px;
  --aside-size:      12px;
  --fig-cap-size:    13px;
  /*-- Main font colors --*/
  --title-color:     #000000;
  --header-color:    rgba(0, 0, 0, 0.8);
  --body-color:      rgba(0, 0, 0, 0.8);
  --aside-color:     rgba(0, 0, 0, 0.6);
  --fig-cap-color:   rgba(0, 0, 0, 0.6);
  /*-- Specify custom fonts ~~~ must be imported above   --*/
  --heading-font:    sans-serif;
  --mono-font:       monospace;
  --body-font:       sans-serif;
  --navbar-font:     sans-serif;  /* websites + blogs only */
}

/*-- ARTICLE METADATA --*/
d-byline {
  --heading-size:    0.6rem;
  --heading-color:   rgba(0, 0, 0, 0.5);
  --body-size:       0.8rem;
  --body-color:      rgba(0, 0, 0, 0.8);
}

/*-- ARTICLE TABLE OF CONTENTS --*/
.d-contents {
  --heading-size:    18px;
  --contents-size:   13px;
}

/*-- ARTICLE APPENDIX --*/
d-appendix {
  --heading-size:    15px;
  --heading-color:   rgba(0, 0, 0, 0.65);
  --text-size:       0.8em;
  --text-color:      rgba(0, 0, 0, 0.5);
}

/*-- WEBSITE HEADER + FOOTER --*/
/* These properties only apply to Distill sites and blogs  */

.distill-site-header {
  --title-size:       18px;
  --text-color:       rgba(255, 255, 255, 0.8);
  --text-size:        15px;
  --hover-color:      white;
  --bkgd-color:       #0F2E3D;
}

.distill-site-footer {
  --text-color:       rgba(255, 255, 255, 0.8);
  --text-size:        15px;
  --hover-color:      white;
  --bkgd-color:       #0F2E3D;
}

/*-- Additional custom styles --*/
/* Add any additional CSS rules below                      */
</style>
<style type="text/css">
/* base variables */

/* Edit the CSS properties in this file to create a custom
   Distill theme. Only edit values in the right column
   for each row; values shown are the CSS defaults.
   To return any property to the default,
   you may set its value to: unset
   All rows must end with a semi-colon.                      */

/* Optional: embed custom fonts here with `@import`          */
/* This must remain at the top of this file.                 */
@import url('https://fonts.googleapis.com/css2?family=Amiri');
@import url('https://fonts.googleapis.com/css2?family=Bitter');
@import url('https://fonts.googleapis.com/css2?family=DM+Mono');


html {
  /*-- Main font sizes --*/
  --title-size:      40px;
  --body-size:       1.06rem;
  --code-size:       14px;
  --aside-size:      12px;
  --fig-cap-size:    13px;
  /*-- Main font colors --*/
  --title-color:     #000000;
  --header-color:    rgba(0, 0, 0, 0.8);
  --body-color:      rgba(0, 0, 0, 0.8);
  --aside-color:     rgba(0, 0, 0, 0.6);
  --fig-cap-color:   rgba(0, 0, 0, 0.6);
  /*-- Specify custom fonts ~~~ must be imported above   --*/
  --heading-font:    sans-serif;
  --mono-font:       monospace;
  --body-font:       sans-serif;
  --navbar-font:     sans-serif;
}

/*-- ARTICLE METADATA --*/
d-byline {
  --heading-size:    0.6rem;
  --heading-color:   rgba(0, 0, 0, 0.5);
  --body-size:       0.8rem;
  --body-color:      rgba(0, 0, 0, 0.8);
}

/*-- ARTICLE TABLE OF CONTENTS --*/
.d-contents {
  --heading-size:    18px;
  --contents-size:   13px;
}

/*-- ARTICLE APPENDIX --*/
d-appendix {
  --heading-size:    15px;
  --heading-color:   rgba(0, 0, 0, 0.65);
  --text-size:       0.8em;
  --text-color:      rgba(0, 0, 0, 0.5);
}

/*-- WEBSITE HEADER + FOOTER --*/
/* These properties only apply to Distill sites and blogs  */

.distill-site-header {
  --title-size:       18px;
  --text-color:       rgba(255, 255, 255, 0.8);
  --text-size:        15px;
  --hover-color:      white;
  --bkgd-color:       #0F2E3D;
}

.distill-site-footer {
  --text-color:       rgba(255, 255, 255, 0.8);
  --text-size:        15px;
  --hover-color:      white;
  --bkgd-color:       #0F2E3D;
}

/*-- Additional custom styles --*/
/* Add any additional CSS rules below                      */
</style>
<style type="text/css">
/* base style */

/* FONT FAMILIES */

:root {
  --heading-default: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  --mono-default: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  --body-default: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

body,
.posts-list .post-preview p,
.posts-list .description p {
  font-family: var(--body-font), var(--body-default);
}

h1, h2, h3, h4, h5, h6,
.posts-list .post-preview h2,
.posts-list .description h2 {
  font-family: var(--heading-font), var(--heading-default);
}

d-article div.sourceCode code,
d-article pre code {
  font-family: var(--mono-font), var(--mono-default);
}


/*-- TITLE --*/
d-title h1,
.posts-list > h1 {
  color: var(--title-color, black);
}

d-title h1 {
  font-size: var(--title-size, 50px);
}

/*-- HEADERS --*/
d-article h1,
d-article h2,
d-article h3,
d-article h4,
d-article h5,
d-article h6 {
  color: var(--header-color, rgba(0, 0, 0, 0.8));
}

/*-- BODY --*/
d-article > p,  /* only text inside of <p> tags */
d-article > ul, /* lists */
d-article > ol {
  color: var(--body-color, rgba(0, 0, 0, 0.8));
  font-size: var(--body-size, 1.06rem);
}


/*-- CODE --*/
d-article div.sourceCode code,
d-article pre code {
  font-size: var(--code-size, 14px);
}

/*-- ASIDE --*/
d-article aside {
  font-size: var(--aside-size, 12px);
  color: var(--aside-color, rgba(0, 0, 0, 0.6));
}

/*-- FIGURE CAPTIONS --*/
figure .caption,
figure figcaption,
.figure .caption {
  font-size: var(--fig-cap-size, 13px);
  color: var(--fig-cap-color, rgba(0, 0, 0, 0.6));
}

/*-- METADATA --*/
d-byline h3 {
  font-size: var(--heading-size, 0.6rem);
  color: var(--heading-color, rgba(0, 0, 0, 0.5));
}

d-byline {
  font-size: var(--body-size, 0.8rem);
  color: var(--body-color, rgba(0, 0, 0, 0.8));
}

d-byline a,
d-article d-byline a {
  color: var(--body-color, rgba(0, 0, 0, 0.8));
}

/*-- TABLE OF CONTENTS --*/
.d-contents nav h3 {
  font-size: var(--heading-size, 18px);
}

.d-contents nav a {
  font-size: var(--contents-size, 13px);
}

/*-- APPENDIX --*/
d-appendix h3 {
  font-size: var(--heading-size, 15px);
  color: var(--heading-color, rgba(0, 0, 0, 0.65));
}

d-appendix {
  font-size: var(--text-size, 0.8em);
  color: var(--text-color, rgba(0, 0, 0, 0.5));
}

d-appendix d-footnote-list a.footnote-backlink {
  color: var(--text-color, rgba(0, 0, 0, 0.5));
}

/*-- WEBSITE HEADER + FOOTER --*/
.distill-site-header .title {
  font-size: var(--title-size, 18px);
  font-family: var(--navbar-font), var(--heading-default);
}

.distill-site-header a,
.nav-dropdown .nav-dropbtn {
  font-family: var(--navbar-font), var(--heading-default);
}

.nav-dropdown .nav-dropbtn {
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  font-size: var(--text-size, 15px);
}

.distill-site-header a:hover,
.nav-dropdown:hover .nav-dropbtn {
  color: var(--hover-color, white);
}

.distill-site-header {
  font-size: var(--text-size, 15px);
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  background-color: var(--bkgd-color, #0F2E3D);
}

.distill-site-footer {
  font-size: var(--text-size, 15px);
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  background-color: var(--bkgd-color, #0F2E3D);
}

.distill-site-footer a:hover {
  color: var(--hover-color, white);
}</style>
<!--/radix_placeholder_distill-->
  <script src="temp/site_libs/header-attrs-2.7/header-attrs.js"></script>
  <script src="temp/site_libs/jquery-1.11.3/jquery.min.js"></script>
  <script src="temp/site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="temp/site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="temp/site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="temp/site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="temp/site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="temp/site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="temp/site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="temp/site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<!--/radix_placeholder_site_in_header-->
  <!DOCTYPE html>
  <html lang="en" dir="ltr">
    <head>
      <meta charset="utf-8">
      <title></title>
      <script>
        /*mouseover & mouseout*/
        function setNewImage(){
          document.getElementById("main_image").src = "assets/teaser/colored_grid.png";
        }
        function setOldImage(){
          document.getElementById("main_image").src = "assets/teaser/grid.png";
        }
      </script>
      <style>
        .article-banner {
        width: auto;
        height: auto;
        border: 0;
        }
        img[src="assets/teaser/colored_grid.png"] {
          border: 0;
          padding: 0;
          width: 60%;
          display: block;
          margin-left: auto;
          margin-right: auto; 
          margin-top: 53px;
        }
        img[src="assets/teaser/grid.png"] {
          border: 0;
          padding: 0;
          width: 60%;
          display: block;
          margin-left: auto;
          margin-right: auto; 
          margin-top: 53px;
        }
        
        figcaption {
          color: grey;
          font-style: italic;
          padding: 2px;
          text-align: center;
        }
        .btn-container {
            width: 60%;
            display: flex;
            justify-content: center;
            margin-left: auto;
            margin-right: auto; 
            z-index: 1000;
            padding-top: 16px;
            position: relative;
        }
            
        .ourbutton {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 150px;
            height: 50px;
            border-radius: 10px;
            /*background-color: #ffffff1f;*/
            background-color:#236b90;
            border: solid 2px white;
            color: white;
            font-size: 18px;
            cursor: pointer;
            margin: 0 5px;
            user-select: none;
            text-align: center;
        }
        .ourbutton:hover {
            /*background-color: rgba(255, 255, 255, 0.17);*/
            background-color:#0F2E3D;
        }
        .ourbutton:after {
          background-color: darken(#0F2E3D, 20%);
        }
      </style>
    </head>
    <body>
      <div class="article-banner">
        <img id="main_image" onmouseover="setNewImage()" onmouseout="setOldImage()" src="assets/teaser/grid.png" class="article-banner";>
        <figcaption>Which images require more/fewer human annotation? Hover to see our model's opinion!</figcaption>
      </div>
      </a>
      <div class="btn-container">
        <div class="ourbutton" onclick="window.open('https://arxiv.org/abs/2104.12690')">Paper</div>
        <div class="ourbutton" onclick="window.open('https://github.com/fidler-lab/efficient-annotation-cookbook')">Code</div>
        <!--div class="ourbutton" onclick="window.open('./demo/index.html')">Demo</div-->
        <div class="ourbutton" onclick="window.open('')">Video (TBA)</div>
      </div>
    </body>
  </html>


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Efficient Annotation Cookbook for Image Classification","description":"Based on the paper \"Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets\", accepted by CVPR21 as Oral presentation.","authors":[{"author":"Yuan-Hong Liao","authorURL":"https://andrewliao11.github.io","affiliation":"University of Toronto, Vector Institute","affiliationURL":"#","orcidID":""},{"author":"Amlan Kar","authorURL":"https://amlankar.github.io","affiliation":"University of Toronto, Vector Institute, Nvidia","affiliationURL":"#","orcidID":""},{"author":"Sanja Fidler","authorURL":"http://www.cs.utoronto.ca/~fidler/","affiliation":"University of Toronto, Vector Institute, Nvidia","affiliationURL":"#","orcidID":""}],"publishedDate":"2021-04-26T00:00:00.000+00:00","citationText":"Liao, et al., 2021"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a href="" class="title">Towards Efficient Annotation</a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<!--div class="nav-right">
<a href="../../index.html">Home</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div-->
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Efficient Annotation Cookbook for Image Classification</h1>
<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>Based on the paper âTowards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets,â accepted by CVPR21 as Oral presentation.</p></p>
</div>

<div class="d-byline">
  Yuan-Hong Liao <a href="https://andrewliao11.github.io" class="uri">https://andrewliao11.github.io</a> (University of Toronto, Vector Institute)
  
,   Amlan Kar <a href="https://amlankar.github.io" class="uri">https://amlankar.github.io</a> (University of Toronto, Vector Institute, Nvidia)
  
,   Sanja Fidler <a href="http://www.cs.utoronto.ca/~fidler/" class="uri">http://www.cs.utoronto.ca/~fidler/</a> (University of Toronto, Vector Institute, Nvidia)
  
<br/>04-06-2021
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#laborious-annotation-process">Laborious Annotation Process</a></li>
<li><a href="#background-and-testbed">Background and Testbed</a>
<ul>
<li><a href="#how-to-aggregate-labels-ds-model">How to aggregate labels: DS model</a></li>
<li><a href="#testbed-imagenet100-sandbox">Testbed: ImageNet100 Sandbox</a></li>
</ul></li>
<li><a href="#matters-of-model-learning">Matters of Model Learning</a>
<ul>
<li><a href="#online-labeling-is-a-semi-supervised-problem">Online-Labeling is a Semi-Supervised Problem</a></li>
<li><a href="#self-supervised-learning-advances-online-labeling">Self-Supervised Learning Advances Online-Labeling</a></li>
<li><a href="#clean-validation-set-matters-in-accuracy-and-calibration-error">Clean Validation set Matters in Accuracy and Calibration Error</a></li>
</ul></li>
<li><a href="#matters-of-workers">Matters of Workers</a>
<ul>
<li><a href="#its-worth-using-gold-standard-question-sometimes">Itâs worth using Gold Standard Question sometimes</a></li>
<li><a href="#tradeoff-between-number-of-workers-and-time-cost">Tradeoff between number of workers and time cost</a></li>
</ul></li>
<li><a href="#matters-of-data">Matters of Data</a>
<ul>
<li><a href="#pre-filtering-dataset-to-some-extent">Pre-filtering Dataset to some extent</a></li>
<li><a href="#early-stopping-saves-you-some-money">Early Stopping Saves you some Money</a></li>
</ul></li>
<li><a href="#analysis">Analysis</a>
<ul>
<li><a href="#greedy-task-assignment-w-estimated-worker-skills">Greedy Task Assignment w/ estimated Worker Skills</a></li>
<li><a href="#how-does-the-annotation-process-look-like">How does the Annotation Process look like</a></li>
</ul></li>
<li><a href="#discussion">Discussion</a>
<ul>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#paper">Paper</a></li>
</ul></li>
</ul>
</nav>
</div>
<p>Data is the engine of modern computer vision, which necessitates collecting large-scale datasets. This is expensive, and guaranteeing the quality of the labels is a major challenge. We investigate <strong>efficient annotation strategies</strong> for collecting multi-class classification labels for a large collection of images.</p>
<p>In this paper, we show that incorporating machine learner to online labeling coupled with several good design choices, we can increase the annotation efficiency significantly. We end up being <strong>2.7x efficient (w/ 63% less annotations)</strong> w.r.t prior work and <strong>6.7x efficient (w/ 85% less annotations)</strong> w.r.t. manual annotations.</p>
<aside>
Shout out to <span class="citation" data-cites="Branson_2017_CVPR"><a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona</a> (<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">2017</a>)</span> that inspires this work.
</aside>
<h1 id="laborious-annotation-process">Laborious Annotation Process</h1>
<p>A common approach used in practice is to query humans to get a fixed number of labels per datum and aggregate them <span class="citation" data-cites="lin2014microsoft kaur2019foodx russakovsky2015imagenet">(<a href="#ref-lin2014microsoft" role="doc-biblioref">Lin et al. 2014</a>; <a href="#ref-kaur2019foodx" role="doc-biblioref">Kaur et al. 2019</a>; <a href="#ref-russakovsky2015imagenet" role="doc-biblioref">Russakovsky et al. 2015</a>)</span> presumably because of its simplicity and reliability. This can be prohibitively expensive and inefficient in human resource utilization for large datasets, as it assumes equal effort needed per datum.</p>
<h1 id="background-and-testbed">Background and Testbed</h1>
<h3 id="how-to-aggregate-labels-ds-model">How to aggregate labels: DS model</h3>
<p>The Dawid-Skene model views the annotation process as jointly inferring true labels and worker skills. The joint probability of true labels <span class="math inline">\(\mathcal{Y}\)</span>, annotations <span class="math inline">\(\mathcal{Z}\)</span>, and worker skills <span class="math inline">\(\mathcal{W}\)</span> is defined as the product of the prior of true labels and worker skills and the posterior of the annotations. We first define the notations, <span class="math inline">\(\mathcal{I_j}\)</span>: the images annotated by the <span class="math inline">\(j^{th}\)</span> worker, <span class="math inline">\(\mathcal{W_i}\)</span>: the workers that annotate the <span class="math inline">\(i^{th}\)</span> image, <span class="math inline">\(N\)</span>: the number of images, <span class="math inline">\(M\)</span>: the number of workers. Now, we can define the joint probability as <span class="math inline">\(P(\mathcal{Y}, \mathcal{Z}, \mathcal{W}) = \prod_{i \in [N]} p(y_i) \prod_{j \in [M]} p(w_j) \prod_{i, j \in \mathcal{W_i}} p(z_{ij} | y_i, w_j)\)</span>. In practice, inference is performed using expectation maximization, where parameters for images or workers are optimized at a time, <span class="math display">\[
\begin{align}
\bar{y_i} &amp;= \arg \max p(y_i) \prod_{j \in \mathcal{W_i}} p(z_{ij} | y_i, \bar{w_j}) \\
\bar{w_j} &amp;= \arg \max p(w_j) \prod_{i \in \mathcal{I_j}} p(z_{ij} | \bar{y_i}, w_j) \\
\end{align}
\]</span></p>
<p>Prior work<span class="citation" data-cites="Branson_2017_CVPR">(<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona 2017</a>)</span> moves to an online setting and improves DS model by using machine learning model predictions as image prior <span class="math inline">\(p(y_i)\)</span>, opening up the window of incorporating machine learner and DS model. However, they only perform experiments in a small-scale setting. We ask ourselves: <strong>How many annotations can we possibly reduce to annotate a large scale image classification dataset, such as ImageNet?</strong></p>
<h3 id="testbed-imagenet100-sandbox">Testbed: ImageNet100 Sandbox</h3>
<p>Evaluating and ablating multi-class label annotation efficiency at scale requires large datasets with diverse and relatively clean labels. We construct multiple subsets of the ImageNet dataset <span class="citation" data-cites="russakovsky2015imagenet">(<a href="#ref-russakovsky2015imagenet" role="doc-biblioref">Russakovsky et al. 2015</a>)</span> for our experiments. The following table shows the details of the different subsets in ImageNet100 Sandbox.</p>
<table>
<thead>
<tr class="header">
<th>Dataset</th>
<th style="text-align: center;">#Images</th>
<th style="text-align: center;">#Classes</th>
<th style="text-align: center;">Worker Acc.</th>
<th style="text-align: center;">Fine-Grained</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Commodity</td>
<td style="text-align: center;">20140</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td>Vertebrate</td>
<td style="text-align: center;">23220</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td>Insect + Fungus</td>
<td style="text-align: center;">16770</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">V</td>
</tr>
<tr class="even">
<td>Dog</td>
<td style="text-align: center;">22704</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">V</td>
</tr>
<tr class="odd">
<td>ImageNet100</td>
<td style="text-align: center;">125689</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">V</td>
</tr>
</tbody>
</table>
<p>Prior work <span class="citation" data-cites="hua2013collaborative long2015multi">(<a href="#ref-hua2013collaborative" role="doc-biblioref">Hua et al. 2013</a>; <a href="#ref-long2015multi" role="doc-biblioref">Long and Hua 2015</a>)</span> simulates workers as confusion matrices. Class confusion was modeled with symmetric uniform noise, which can result in <em>over-optimistic</em> performance estimates. Human annotators exhibit asymmetric and structured confusion <em>i.e.</em>, classes get confused with each other differently. In Fig.<a href="#fig:structured-workers">1</a>, we compare the number of annotations per image in simulation using uniform label noise vs.Â structured label noise that we crowdsource. We see significant gaps between the two. This arises particularly when using learnt models in the loop due to sensitivity to noisy labels coming from structured confusion in the workers. Therefore, we use simulated workers with structured noise in ImageNet100 Sandbox.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:structured-workers"></span>
<img src="assets/paper_figures/plot_unfiorm_structured_noise_pseudolabel.png" alt="Over-optimistic results from workers with uniform noise. Human workers tend to make *structured* mistakes. Simulated workers with uniform label noise (blue) can result in over-optimistic annotation performance. Experiments under workers with structured noise reflect real-life performance better." width="2098" />
<p class="caption">
Figure 1: Over-optimistic results from workers with uniform noise. Human workers tend to make <em>structured</em> mistakes. Simulated workers with uniform label noise (blue) can result in over-optimistic annotation performance. Experiments under workers with structured noise reflect real-life performance better.
</p>
</div>
</div>
<p>We simulate the process of annotating ImageNet100 and perform various ablation on the system, spanning from models, workers, and data itself. We end up being <strong>2.7x efficient (w/ 63% less annotations)</strong> w.r.t prior work <span class="citation" data-cites="Branson_2017_CVPR">(<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona 2017</a>)</span> and <strong>6.7x efficient (w/ 85% less annotations)</strong> w.r.t. manual annotations <span class="citation" data-cites="dawid1979maximum">(<a href="#ref-dawid1979maximum" role="doc-biblioref">Dawid and Skene 1979</a>)</span>.</p>
<p>In the following, we show how each component affects the final efficiency:</p>
<h1 id="matters-of-model-learning">Matters of Model Learning</h1>
<h3 id="online-labeling-is-a-semi-supervised-problem">Online-Labeling is a Semi-Supervised Problem</h3>
<p>During online-labeling, the goal is to infer true labels for all images in the dataset, making model learning akin to transductive learning <span class="citation" data-cites="joachims1999transductive">(<a href="#ref-joachims1999transductive" role="doc-biblioref">Joachims 1999</a>)</span>, where the test set is observed and can be used for learning. Thus, it is reasonable to expect efficiency gains if the datasetâs underlying structure is exploited by putting the unlabeled data to work, using semi-supervised learning. In Fig.<a href="#fig:semi-supervised">2</a> we perform <em>Peudolabel</em> and <em>Mixmatch</em> in online-labeling.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:semi-supervised"></span>
<img src="assets/paper_figures/plot_semi.png" alt="Incorporating semi-supervised approaches consistently increases efficiency. Note that semi-supervised learning does not have a significant boost in Dog subset due ot the poor worker quality (43%). When eyeballing the subset, we also find ineligible label errors in the Dog subset." width="2098" />
<p class="caption">
Figure 2: Incorporating semi-supervised approaches consistently increases efficiency. Note that semi-supervised learning does not have a significant boost in Dog subset due ot the poor worker quality (43%). When eyeballing the subset, we also find ineligible label errors in the Dog subset.
</p>
</div>
</div>
<h3 id="self-supervised-learning-advances-online-labeling">Self-Supervised Learning Advances Online-Labeling</h3>
<p>With recent advances in self-supervised learning, it is feasible to learn strong image feature extractors that rival supervised learning, using pretext tasks without any label. This allows learning in-domain feature extractors for annotation tasks, as opposed to using features pre-trained on ImageNet We compare the efficacy of using BYOL <span class="citation" data-cites="grill2020bootstrap">(<a href="#ref-grill2020bootstrap" role="doc-biblioref">Grill et al. 2020</a>)</span>, SimCLR <span class="citation" data-cites="chen2020simple">(<a href="#ref-chen2020simple" role="doc-biblioref">T. Chen et al. 2020</a>)</span>, MoCo <span class="citation" data-cites="he2019moco">(<a href="#ref-he2019moco" role="doc-biblioref">He et al. 2019</a>)</span>, relative location prediction <span class="citation" data-cites="doersch2015unsupervised">(<a href="#ref-doersch2015unsupervised" role="doc-biblioref">Doersch, Gupta, and Efros 2015</a>)</span> and rotation prediction <span class="citation" data-cites="gidaris2018unsupervised">(<a href="#ref-gidaris2018unsupervised" role="doc-biblioref">Gidaris, Singh, and Komodakis 2018</a>)</span> learnt on full ImageNet raw images as the feature extractor. In Fig.<a href="#fig:self-supervised">3</a>, we show that improvements in self-supervised learning consistently increase the efficiency for datasets with both fine and coarse-grained labels, with up to <strong>5x improvement</strong> at similar accuracy compared to not using a machine learning model in the loop (online DS).</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:self-supervised"></span>
<img src="assets/paper_figures/plot_self_none.png" alt="The improvements in self-supervised learning can be translated seemingly to online labeling. Note that no semi-supervised tricks are applied in this figure." width="2098" />
<p class="caption">
Figure 3: The improvements in self-supervised learning can be translated seemingly to online labeling. Note that no semi-supervised tricks are applied in this figure.
</p>
</div>
</div>
<h3 id="clean-validation-set-matters-in-accuracy-and-calibration-error">Clean Validation set Matters in Accuracy and Calibration Error</h3>
<p>The validation set plays an important role in online-labeling. It is used to perform model selection and model calibration. Prior work <span class="citation" data-cites="Branson_2017_CVPR">(<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona 2017</a>)</span> uses a modified cross-validation approach to generate model likelihoods. We find that this could underperform when the estimated labels are noisy, which pollutes the validation set and makes calibration challenging. Instead, we propose to use the clean prototype images as the validation set. In our paper, we use 10 prototype images per class. We perform 3-fold cross-validation in this experiment. When not using cross-validation, we either randomly select a subset as the validation set or use the (clean) prototype images as the validation set. In Fig.<a href="#fig:clean-validation">4</a>, we ablate the importance of having a clean validation set and performing cross-validation in terms of accuracy and expected calibration error on the most challenging subset, the Dog subset.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:clean-validation"></span>
<img src="assets/paper_figures/plot_prototype_as_val.png" alt="The validation set plays an important role of online-labeling. It is used to perform model selection and model calibration. We compare the importance of using clean examples as the validation set. When not using a clean validation set, the model tends to produce poorly calibrated probability (w/ calibration method applied [@guo2017calibration]), resulting in poor accuracy." width="1316" />
<p class="caption">
Figure 4: The validation set plays an important role of online-labeling. It is used to perform model selection and model calibration. We compare the importance of using clean examples as the validation set. When not using a clean validation set, the model tends to produce poorly calibrated probability (w/ calibration method applied <span class="citation" data-cites="guo2017calibration">(<a href="#ref-guo2017calibration" role="doc-biblioref">Guo et al. 2017</a>)</span>), resulting in poor accuracy.
</p>
</div>
</div>
<h1 id="matters-of-workers">Matters of Workers</h1>
<h3 id="its-worth-using-gold-standard-question-sometimes">Itâs worth using Gold Standard Question sometimes</h3>
<p>In reality, the requestor can ask gold standard questions or apply prior knowledge to design the prior <span class="math inline">\(p(w_j)\)</span>. We explore two possible prior <strong>A)</strong> Considering class identity and <strong>B)</strong> Considering worker identity. To consider the class identity, the task designer needs to have a clear thought of which classes are more difficult than others. To consider the workerâs identity, the task designer needs to query several gold standard questions from each worker. In Fig.<a href="#fig:prior-change">5</a>, we find that considering worker identity is especially useful for fine-grained datasets, such as Dog subset, improving 15 accuracy points in Dog, while in Commodity, the improvement is marginal.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:prior-change"></span>
<img src="assets/paper_figures/plot_prior_change-10.png" alt="For the fine-grained dataset, it is usually worth using gold standard questions to get a better prior over worker skills. The number appended in the legend denotes the prior strength." width="2098" />
<p class="caption">
Figure 5: For the fine-grained dataset, it is usually worth using gold standard questions to get a better prior over worker skills. The number appended in the legend denotes the prior strength.
</p>
</div>
</div>
<h3 id="tradeoff-between-number-of-workers-and-time-cost">Tradeoff between number of workers and time cost</h3>
<p>One way to speed up the dataset annotation process is to hire more workers at the same time. However, under a fixed number of total annotations, having more workers means having fewer observations for each worker, resulting in poor worker skill estimation. We explore this tradeoff by manipulating the number of workers involved in Fig.<a href="#fig:number-workers">6</a>. The gap is surprisingly high in the fine-grained dataset (14% accuracy points difference in Dog subset), while there is nearly no tradeoff in the coarse-grained dataset, such as Commodity subset.</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:number-workers"></span>
<img src="assets/paper_figures/plot_worker_change.png" alt="Hiring more workers saves the total time to annotate a dataset, while it sacrifices the accuracy of the dataset sometimes. For the fine-grained dataset, the gap between using 10 workers and 1000 workers is around 14 accuracy points, while in the coarse-grained dataset, there is nearly no tradeoff." width="2098" />
<p class="caption">
Figure 6: Hiring more workers saves the total time to annotate a dataset, while it sacrifices the accuracy of the dataset sometimes. For the fine-grained dataset, the gap between using 10 workers and 1000 workers is around 14 accuracy points, while in the coarse-grained dataset, there is nearly no tradeoff.
</p>
</div>
</div>
<h1 id="matters-of-data">Matters of Data</h1>
<h3 id="pre-filtering-dataset-to-some-extent">Pre-filtering Dataset to some extent</h3>
<p>We have assumed that the requestor performs perfect filtering before annotation, <em>i.e.</em>, all the images to be annotated belong to the target classes, which does not always hold. We add an additional âNone of Theseâ class and ablate annotation efficiency in the presence of unfiltered images. We include different numbers of images from other classes and measure the mean precision with the number of annotations of the target classes. In Fig.<a href="#fig:distraction-image">7</a>, we see that even with 100% more images from irrelevant classes, we can retain comparable efficiency on a fine-grained dataset.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:distraction-image"></span>
<img src="assets/paper_figures/plot_distraction_compare_cost_target_classes_precision.png" alt="We intentionally add some irrelevance images from other classes to mimic the real-world cases. In our experiments, we find that even with 100% more images from irrelevant classes, we can still retain comparable efficiency." width="50%" />
<p class="caption">
Figure 7: We intentionally add some irrelevance images from other classes to mimic the real-world cases. In our experiments, we find that even with 100% more images from irrelevant classes, we can still retain comparable efficiency.
</p>
</div>
</div>
<h3 id="early-stopping-saves-you-some-money">Early Stopping Saves you some Money</h3>
<p>A clear criterion to stop annotation is when the unfinished set of images (images with estimated risk greater than a threshold) is empty. However, we observe that the annotation accuracy usually saturates and then grows slowly because of a small number of data points that are heavily confused by the pool of workers used. Therefore we suggest that the requestor 1) stop annotation at this time and separately annotate the small number of unfinished samples, possibly with expert annotators, and 2) set a maximum number of annotations per image. In Fig.<a href="#fig:early-stopping">8</a>, we show this is sufficient. We set the maximum annotations of each example to be 3 and early stop when the size of the finished set does not increase from its maximum value for 5 consecutive steps.</p>
<aside>
In online-labeling, we estimate the risk for each example at every step. If the exampleâs risk satisfies pre-defined threshold, we put them into finished set and will not query its annotation from workers at the next time step.
</aside>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:early-stopping"></span>
<img src="assets/paper_figures/plot_stopping.png" alt="We perform early stopping by monitoring the size of the finished set. This avoids over-sampling for confusing images and leaves the rest of them to expert workers if possible. Dashed lines represent the trajectories using stopping criterion from prior work [@Branson_2017_CVPR]" width="50%" />
<p class="caption">
Figure 8: We perform early stopping by monitoring the size of the finished set. This avoids over-sampling for confusing images and leaves the rest of them to expert workers if possible. Dashed lines represent the trajectories using stopping criterion from prior work <span class="citation" data-cites="Branson_2017_CVPR">(<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona 2017</a>)</span>
</p>
</div>
</div>
<h1 id="analysis">Analysis</h1>
<h3 id="greedy-task-assignment-w-estimated-worker-skills">Greedy Task Assignment w/ estimated Worker Skills</h3>
<p>There are certain particularly hard classes, with only a few workers having enough expertise to annotate them correctly. We ask whether the learnt skills can be used to assign tasks better. Prior work on (optimal) task assignment tackle crowdsourcing settings with vastly different simplifying assumptions <span class="citation" data-cites="ho2013adaptive chen2015statistical">(<a href="#ref-ho2013adaptive" role="doc-biblioref">Ho, Jabbari, and Vaughan 2013</a>; <a href="#ref-chen2015statistical" role="doc-biblioref">X. Chen, Lin, and Zhou 2015</a>)</span>, and designing a new task assignment scheme is out of the scope of this paper. We verify if the learnt worker skills help with task assignment by proposing a simple greedy algorithm with a cap on the maximum number of annotations per worker. Fig.<a href="#fig:task-assignment">9</a> shows that the above-mentioned task assignment scheme consistently improves over the random assignment, implying that the learnt skills are both representative and good enough to bring some improvements.</p>
<p>Fig.<a href="#fig:taskassignment-vis">10</a> visualizes the workerâs importance versus the number of annotations per worker. Since there is no exploration in the proposed task assignment, the workers are separated into two groups: overwork (top right) and underwork (bottom left).</p>
<div class="layout-chunk" data-layout="l-body-outset">
<div class="figure"><span id="fig:task-assignment"></span>
<img src="assets/paper_figures/plot_task_assignment.png" alt="The simple greedy task assignment scheme consistently improves over random assignment both in fine-grained and coarse-grained subsets." width="2098" />
<p class="caption">
Figure 9: The simple greedy task assignment scheme consistently improves over random assignment both in fine-grained and coarse-grained subsets.
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:taskassignment-vis"></span>
<img src="assets/paper_figures/plot_task_assignment_vis.png" alt="Since there is no exploration in the proposed task assignment, the workers are separated into two groups: overwork (top right) and underwork (bottom left)." width="1316" />
<p class="caption">
Figure 10: Since there is no exploration in the proposed task assignment, the workers are separated into two groups: overwork (top right) and underwork (bottom left).
</p>
</div>
</div>
<h3 id="how-does-the-annotation-process-look-like">How does the Annotation Process look like</h3>
<p>To further see how the annotation process is like, we sample 100 data points on the Commodity subset and visualize the risk change and accuracy change through time. In the top row of Fig.<a href="#fig:commodity-change">11</a>, we show the risk of the aggregated probability <span class="math inline">\(p_i\)</span> and the risk of the machine learnerâs probability <span class="math inline">\(p_i^{\text{ML}}\)</span>. Each column represents one data point. We can see from the aggregated probability (top-left) that the risk gradually decreases as time goes by. Comparing the risk of <span class="math inline">\(p_i\)</span> and <span class="math inline">\(p_i^{\text{ML}}\)</span>, we find that the annotation process can be separated into two stages. In the first stage, the machine learner dominates the progress, while in the second stage, the worker annotations dominate it.</p>
<p>In the bottom row of Fig.<a href="#fig:commodity-change">11</a>, we show the correctness of each example of <span class="math inline">\(p_i\)</span> and <span class="math inline">\(p_i^{\text{ML}}\)</span>. This coincides with the trend mentioned above. The reason that the machine learner saturates quickly might be due to the lack of expressivity. In our work, our machine learner only fine-tunes the last few layers.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span id="fig:commodity-change"></span>
<img src="assets/paper_figures/imagenet_split_4-proability_change.png" alt="Top row: the risk of the aggregated probability and machine learner's probability. Bottom row: Correctness of the aggregated probability and machine learner's probability. Comparing the left and right columns, we find that the annotation process can be separated into two stages. In the first stage, the machine learner dominates the progress, while in the second stage, the worker annotations dominate it." width="629" />
<p class="caption">
Figure 11: Top row: the risk of the aggregated probability and machine learnerâs probability. Bottom row: Correctness of the aggregated probability and machine learnerâs probability. Comparing the left and right columns, we find that the annotation process can be separated into two stages. In the first stage, the machine learner dominates the progress, while in the second stage, the worker annotations dominate it.
</p>
</div>
</div>
<h1 id="discussion">Discussion</h1>
<p>We presented improved online-labeling methods for large multi-class datasets. In a realistically simulated experiment with 125k images and 100 labels from ImageNet, we observe a 2.7x reduction in annotations required w.r.t. prior work to achieve 80% top-1 label accuracy. Our framework goes on to achieve 87.4% top-1 accuracy at 0.98 labels per image. Along with our improvements, we leave open questions for future research. 1) Our simulation is not perfect and does not consider individual image difficulty, instead only modeling class confusion. 2) How does one accelerate labeling beyond semantic classes, such as classifying the viewing angle of a car? 3) ImageNet has a clear label hierarchy, which can be utilized to achieve orthogonal gains <span class="citation" data-cites="van2018lean">(<a href="#ref-van2018lean" role="doc-biblioref">Van Horn et al. 2018</a>)</span> in the worker skill estimation 4) Going beyond classification is possible with the proposed model by appropriately modeling annotation likelihood as demonstrated in <span class="citation" data-cites="Branson_2017_CVPR">(<a href="#ref-Branson_2017_CVPR" role="doc-biblioref">Branson, Van Horn, and Perona 2017</a>)</span>. However, accelerating these with learning in the loop requires specific attention to detail per task, which is an exciting avenue for future work. 5) Finally, we discussed annotation at scale, where improvements in learning help significantly. How can these be translated to small datasets?</p>
<h2 class="appendix" id="acknowledgments">Acknowledgments</h2>
<p>This work was supported by ERA, NSERC, and DARPA XAI. SF acknowledges the Canada CIFAR AI Chair award at the Vector Institute.</p>
<p>This webpage is based on Distill Template generated from <a href="https://rstudio.github.io/distill/">here</a></p>
<h2 class="appendix" id="paper">Paper</h2>
<div class="layout-chunk" data-layout="l-body-outset">
<p><a href="https://arxiv.org/abs/2104.12690"><img src="assets/thumbnail.png" width="885" /></a></p>
</div>
<p><strong>This paper is accepted to CVPR2021 as Oral presentations.</strong></p>
<p>If you find this article useful or you use our code, please consider cite:</p>
<pre><code>@misc{liao2021good,
  title={Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets}, 
  author={Yuan-Hong Liao and Amlan Kar and Sanja Fidler},
  year={2021},
  eprint={2104.12690},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}</code></pre>
<div class="sourceCode" id="cb2"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Branson_2017_CVPR" class="csl-entry" role="doc-biblioentry">
Branson, Steve, Grant Van Horn, and Pietro Perona. 2017. <span>âLean Crowdsourcing: Combining Humans and Machines in an Online System.â</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
</div>
<div id="ref-chen2020simple" class="csl-entry" role="doc-biblioentry">
Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. <span>âA Simple Framework for Contrastive Learning of Visual Representations.â</span> <em>arXiv Preprint arXiv:2002.05709</em>.
</div>
<div id="ref-chen2015statistical" class="csl-entry" role="doc-biblioentry">
Chen, Xi, Qihang Lin, and Dengyong Zhou. 2015. <span>âStatistical Decision Making for Optimal Budget Allocation in Crowd Labeling.â</span> <em>The Journal of Machine Learning Research</em> 16 (1): 1â46.
</div>
<div id="ref-dawid1979maximum" class="csl-entry" role="doc-biblioentry">
Dawid, Alexander Philip, and Allan M Skene. 1979. <span>âMaximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm.â</span> <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em> 28 (1): 20â28.
</div>
<div id="ref-doersch2015unsupervised" class="csl-entry" role="doc-biblioentry">
Doersch, Carl, Abhinav Gupta, and Alexei A Efros. 2015. <span>âUnsupervised Visual Representation Learning by Context Prediction.â</span> In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 1422â30.
</div>
<div id="ref-gidaris2018unsupervised" class="csl-entry" role="doc-biblioentry">
Gidaris, Spyros, Praveer Singh, and Nikos Komodakis. 2018. <span>âUnsupervised Representation Learning by Predicting Image Rotations.â</span> <em>arXiv Preprint arXiv:1803.07728</em>.
</div>
<div id="ref-grill2020bootstrap" class="csl-entry" role="doc-biblioentry">
Grill, Jean-Bastien, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020. <span>âBootstrap Your Own Latent: A New Approach to Self-Supervised Learning.â</span> <em>arXiv Preprint arXiv:2006.07733</em>.
</div>
<div id="ref-guo2017calibration" class="csl-entry" role="doc-biblioentry">
Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. <span>âOn Calibration of Modern Neural Networks.â</span> <em>arXiv Preprint arXiv:1706.04599</em>.
</div>
<div id="ref-he2019moco" class="csl-entry" role="doc-biblioentry">
He, Kaiming, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2019. <span>âMomentum Contrast for Unsupervised Visual Representation Learning.â</span> <em>arXiv Preprint arXiv:1911.05722</em>.
</div>
<div id="ref-ho2013adaptive" class="csl-entry" role="doc-biblioentry">
Ho, Chien-Ju, Shahin Jabbari, and Jennifer Wortman Vaughan. 2013. <span>âAdaptive Task Assignment for Crowdsourced Classification.â</span> In <em>International Conference on Machine Learning</em>, 534â42.
</div>
<div id="ref-hua2013collaborative" class="csl-entry" role="doc-biblioentry">
Hua, Gang, Chengjiang Long, Ming Yang, and Yan Gao. 2013. <span>âCollaborative Active Learning of a Kernel Machine Ensemble for Recognition.â</span> In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 1209â16.
</div>
<div id="ref-joachims1999transductive" class="csl-entry" role="doc-biblioentry">
Joachims, Thorsten. 1999. <span>âTransductive Inference for Text Classification Using Support Vector Machines.â</span> In <em>ICML</em>, 200â209.
</div>
<div id="ref-kaur2019foodx" class="csl-entry" role="doc-biblioentry">
Kaur, Parneet, Karan Sikka, Weijun Wang, Serge Belongie, and Ajay Divakaran. 2019. <span>âFoodx-251: A Dataset for Fine-Grained Food Classification.â</span> <em>arXiv Preprint arXiv:1907.06167</em>.
</div>
<div id="ref-lin2014microsoft" class="csl-entry" role="doc-biblioentry">
Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C Lawrence Zitnick. 2014. <span>âMicrosoft Coco: Common Objects in Context.â</span> In <em>European Conference on Computer Vision</em>, 740â55. Springer.
</div>
<div id="ref-long2015multi" class="csl-entry" role="doc-biblioentry">
Long, Chengjiang, and Gang Hua. 2015. <span>âMulti-Class Multi-Annotator Active Learning with Robust Gaussian Process for Visual Recognition.â</span> In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 2839â47.
</div>
<div id="ref-russakovsky2015imagenet" class="csl-entry" role="doc-biblioentry">
Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. <span>âImageNet Large Scale Visual Recognition Challenge.â</span> <a href="http://arxiv.org/abs/1409.0575">http://arxiv.org/abs/1409.0575</a>.
</div>
<div id="ref-van2018lean" class="csl-entry" role="doc-biblioentry">
Van Horn, Grant, Steve Branson, Scott Loarie, Serge Belongie, and Pietro Perona. 2018. <span>âLean Multiclass Crowdsourcing.â</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2714â23.
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="updates-and-corrections">Corrections</h3>
  <p>If you see mistakes or want to suggest changes, please <a href="https://github.com/fidler-lab/efficient-annotation-cookbook/issues/new">create an issue</a> on the source repository.</p>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
